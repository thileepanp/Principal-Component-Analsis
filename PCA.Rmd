---
title: "Principal Component Analysis"
author: "Thileepan Paulraj"
date: "18 December 2018"
output: pdf_document
---

# UNDERSTANDING PCA (work reproduced from this webpage:https://goo.gl/Wgeieb)

While performing principal components analysis the original n-dimensional data get's projected onto several planes and the plane which has captured the maximum variance of the data becomes the first pricipal component. We select as many principal components as possible so that the maximum variance of the data is captured in minimal number of principal components. 

Maximum variance is captured when the mean squared error between the original dataset and it's projection on to a plane is minimum. 

###Reading data

Let's use the diamonds data set now.  

```{r}
data = read.csv('diamonds.csv')
```

###Viewing all the variable names in the dataset

```{r}
colnames(data)
```

###Taking only the numeric variables so we could use it in our analysis

```{r}
library(dplyr)
data_for_pca <- select(data, -X, -cut, -color, -clarity)
```

## Principal components

Installing the factominer package for PCA
```{r}
library(FactoMineR)
```

```{r}
pca = PCA(data_for_pca)
```
Here,the variables, 'price', 'carat', 'x', 'y', and 'z' form a composite variable called the Principal component 1 or Dim 1 which explains 68.06% of the variance in the data. Variable 'depth' explains 18.37% of the variance in the data along the second dimension. The variable 'table' is in the third dimension. 


```{r}
pca$eig
```

## Eigen values
In the table above, eigen values indicate how much variance each principal component explains. By doing this, the eigen values indicate which principal component is the most important. If we sort the eigen values and the corresponding eigen vectors in decending order, then we will know which principal components capture maximum variance. 
For example if we divide the first eigen value 4.763 by the total of all other eigen values then we will get a percentage of variance of 68.055. This indicates that the first prinipal component will capture 68.055% of total variance in the data. 

## Eigen Vectors

Eigen vectors transform our original data into principal components.  Matrix multiplication of our scaled and centred original dataset with eigen vector number 1 will generate data for principal component 1. Each of these components are projected in a different direction in the 3-D space. The principal components are orthoginal to each other.  

## How much of each variable is represented in each principal component

Now, let's see how much variance of each variable is explained by each principal component. 

```{r}
Correlation_Matrix = as.data.frame(round(cor(data_for_pca,pca$ind$coord)^2*100,0))
Correlation_Matrix[with(Correlation_Matrix, order(-Correlation_Matrix[,1])),]
```

This correlation matric tells us that 98% of the information in carat and X variables are loaded in the first dimension, 95% of the information from y and z variables are loaded in the first dimension. Information from the variables table and depth is spread between dimension 2 and 3. Information of the variable price is spread between 1st and 4th principal components.

If we discard the 5th principal component we will only loose 3% of the information from only 2 variables. Therefore it is safe is discard this dimension and only keep the 4 remaining dimensions.

# COMPUTING PRINCIPAL COMPONENT ANALYSIS STEP BY STEP

Let's use the iris data set to perform pricipal component analysis

```{r}
df <- iris[, -5]
```

Let's centre and scale the data

```{r}
df.scaled <- scale(df, center = TRUE, scale = TRUE)
```

## Computing the correlation matrix

The correlation matrix finds how each variable in our dataset is correlated with the other variables. The diagonals of the correlation matrix will always contain 1s because it's the value of correlation of the variable with itself.

The dimension of the correlation matrix will depend on the number of variables/ features present in our data set. If we have 4 variables then the correlation matrix will be [4,4] in dimension. If we have 16 variables then the correlation matrix will be [16,16] in dimension.  

```{r}
res.cor = cor(df.scaled)
round(res.cor, 2)
```

## Calculating Eigen Values and Eigen Vectors

```{r}
res.eig = eigen(res.cor)

res.eig
```

## Calculating principal components

As mentioned under the heading 'Eigen Vector' above, to calculate principal components, we need to multiply the scaled and centred data set with eigen vectors. Multiplying the first eigen vector with the data matrix gives us the first principal component and multiplying the data matrix with the second eigen vector gives us the second principal component and so on and so forth. 

### Transposing the eigen vectors
```{r}
eigenvectors.t = t(res.eig$vectors)
```

### Transposing the adjusted data 

```{r}
df.scaled.t = t(df.scaled)
```

### Calculating the new principal components

```{r}
pc = eigenvectors.t %*% df.scaled.t
pc = t(pc)
colnames(pc) <- c("PC1", "PC2", "PC3", "PC4")
head(pc)
```

